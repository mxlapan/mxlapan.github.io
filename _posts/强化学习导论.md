---
title: "【RL】强化学习导论"
date: 2025-11-18 17:00 +0800
categories: [RL]
tags: [RL]
---


人类智能的本质吸引了科学家和研究人员的特别兴趣，因为人类是地球上最智能的物种。人类在与周围环境互动时适应其行为的方式是一种至关重要的学习范式，这可以通过一个人如何驾驶汽车来展示。

在学习驾驶时，没有经验的驾驶员会根据自己对交通环境的感知，不断地进行转向、加速和制动操作。基于大脑中内置的学习机制，驾驶员通过**选择受奖励的行为**和**避免受惩罚的行为**来提高自己的技能。经过几个月的不断练习，驾驶员最终能够在日常驾驶操控中合理地考虑安全性、机动性和乘坐舒适性。这种试错过程是人类学习新技能和发展智能的基本方式。

<aside>
💡

**强化学习（RL）**是一种受生物启发的学习方法，专注于为动态环境寻找最优决策或控制策略。传统的强化学习方法侧重于从与未知环境的交互中学习，而现代强化学习方法更像是一个最优控制问题的全空间优化器，它通过最大化或最小化某个标准来搜索最优策略。

</aside>

对人类而言，如何学习一项新技能是最基本的问题之一。这也是人工智能领域的一个基本问题。强化学习的力量主要在于其解决复杂和大规模序贯决策问题的强大能力。由于其在开发超人智能策略方面的潜力，强化学习在**自动驾驶**、**游戏AI**、**机器人控制**和**量化交易**等多个跨学科领域引起了广泛关注。

<aside>
🏆

**2017年**，强化学习被《麻省理工科技评论》列为**十大突破性技术之一**。

</aside>

---

## 1.1 强化学习的历史

强化学习的历史可以追溯到控制论的早期。它的发展主要受到两个独立领域的影响：

1. **动态规划（Dynamic Programming）**
2. **试错学习（Trial-and-Error Learning）**

**动态规划（DP）**由 Richard Bellman 在1950年代首次提出。DP 是最优控制的三大支柱之一，另外两个是变分法和庞特里亚金最大值原理。DP 的基础是将一个多阶段优化问题分解为一系列单阶段问题。

作为对人类和动物学习模式的抽象，**试错学习**尤其受到巴甫洛夫条件反射和工具性条件反射的启发，这两者植根于心理学和神经科学领域。

### 📅 强化学习发展的历史里程碑

| **年份** | **贡献者** | **里程碑** |
| --- | --- | --- |
| 1977 | P. Werbos | 带有函数逼近的动态规划 |
| 1988 | R. Sutton | 时序差分学习 |
| 1992 | C. Watkins | Q-learning |
| 1994 | G. Rummery, M. Niranjan | SARSA |
| 1996 | D. Bertsekas & J. Tsitsiklis | 强化学习的收敛性与稳定性 |
| 1999 | R. Sutton, D. McAllester, S. Singh | 策略梯度 |
| 2014 | D. Silver, G. Lever, N. Heess, et al. | 确定性策略梯度 |
| 2015 | V. Mnih, K. Kavukcuoglu, D. Silver, et al. | 深度Q网络 |
| 2015至今 | — | Atari AI, AlphaGo, MuZero, 赛车AI的成功 |

强化学习中的术语来自多个领域符号的组合，包括**最优控制**、**凸优化**和**统计学习**。在这个迷人的领域，有一些优秀的文献综述和参考书籍可供研究人员使用，包括 Bertsekas 和 Tsitsiklis (1996)、Sutton 和 Barto (1998, 2018)、Busoniu, et al. (2009)、Powell (2011)、Zhang, et al. (2013)、Littman (2015)、Liu, et al. (2017)、Bertsekas (2017)、Dong, et al. (2020)、Meyn (2021) 等。

<aside>
📚

然而，文献中似乎缺少的是一种**自成体系且相对深入的材料**，以帮助新学习者很好地理解强化学习原理的理论基础，并对主流算法有广泛的看法。本书的主要目的之一就是填补这一空白。

</aside>

---

### 1.1.1 动态规划

**最优控制**解决的是为给定动态系统找到一个控制律，以最小化特定准则的问题。最优控制有三大支柱：

- **变分法**（Calculus of Variations）
- **庞特里亚金最大值原理**（Pontryagin's Maximum Principle）
- **动态规划**（Dynamic Programming）

这些理论的发展主要归功于1950年代的 Lev Pontryagin 和 Richard Bellman，以及18世纪的 Leonhard Euler 和 Joseph-Louis Lagrange 的工作。

### 变分法

**变分法**是数学分析的一个子领域，它使用变分来寻找泛函的极大值和极小值。

- **1733年**，Euler 首次通过部分几何的方法阐述了这一主题
- **1755年**，Lagrange 进一步概述了他的 δ-算法，这导出了著名的**欧拉-拉格朗日方程**

该方程是一个二阶偏微分方程，其解是平稳泛函的极值。

### 庞特里亚金最大值原理

**庞特里亚金最大值原理（PMP）**是欧拉-拉格朗日方程在存在输入约束时的扩展。

- 由 Lev Pontryagin 在**1956年**首次提出
- 该原理指出哈密尔顿量必须在所有允许的动作集合上取极值
- 当沿着一条轨迹满足时，PMP是最优的**必要条件**

### 贝尔曼最优性原理

<aside>
⭐

**贝尔曼最优性原理**：一个最优策略具有这样的性质：无论初始状态和初始决策是什么，其余的决策必须构成一个相对于由第一个决策产生的状态的最优策略。

</aside>

这一陈述允许我们将一个**多阶段优化问题**分解为一些初始选择及其后续问题的收益。该原理在最优控制中的应用导出了两个著名的方程：

| **类型** | **连续时间** | **离散时间** |
| --- | --- | --- |
| **必要且充分条件** | 汉密尔顿-雅可比-贝尔曼 (HJB) 方程 | 贝尔曼方程 |

<aside>
⚠️

**挑战**：在实践中，直接求解这些方程通常在计算上是不可行的，因为DP在连续空间和高维系统中会遭受**维度灾难**。

</aside>

**1977年**，Paul Werbos 建议参数化连续状态空间并在参数空间中重构DP。这一思想后来演变成了今天的**近似动态规划（ADP）**，它已广泛用于基于模型的问题。

在构建了参数化策略后，优化维度降低到参数空间的维度。因此，ADP 部分避免了过多的计算需求，尽管它牺牲了一定程度的策略准确性。

大多数 ADP 算法属于**间接方法**家族，其目标是求解贝尔曼方程或 HJB 方程以找到最优策略。这些算法可以进一步分为两类：

- **策略迭代**（Policy Iteration）
- **价值迭代**（Value Iteration）

Bertsekas 和 Tsitsiklis 在1990年代的研究巩固了 ADP 算法设计和收敛性的理论分析。他们的研究主要集中在具有确定性状态空间模型的基于模型的最优控制问题，而不是模型未知的随机环境中的无模型序贯决策问题。

---

### 1.1.2 试错学习

<aside>
🧪

**试错学习**是人类和动物获得智能行为的核心机制之一。通过对相同情况尝试不同的反应，人类和动物可以学习到什么样的行为是好的，什么样的行为是坏的，因此他们可以重复好的行为并避免坏的行为。

</aside>

最终，试错机制使人类和动物能够**预测未来的奖励**并**选择最佳行动**。在人工智能领域，这一思想启发了许多早期强化学习算法的发展，包括**蒙特卡洛学习**和**时序差分学习**。

### 早期发展

- **1952年**，Claude Shannon 介绍了一只机械鼠，它使用试错法在迷宫中寻找路径
- **1960年代早期**，Donald Michie 设计了一个名为 **MENACE** 的机械玩家来玩井字游戏

MENACE 的学习方式与今天的蒙特卡洛（MC）学习器非常相似：

- 每当 MENACE 赢得一场游戏时，这些走法在后续游戏中被更多地使用
- 每当它输掉一场游戏时，这些走法在未来被更少地使用

### "带批评家学习"

Widrow等人 (1973) 设计了一种试错学习规则，可以从成功和失败信号中学习，而不是从标记的训练数据中学习。该算法被称为**"带批评家学习"**（learning with a critic），而不是"带教师学习"（learning with a teacher）。

从那时起，**"批评家"**（critic）这个术语出现在文献中，它类似于一个评估动作表现的外部专家。

<aside>
💡

事实上，试错学习者将反应与刺激联系起来，不涉及任何思考、推理或理解。试错法植根于神经心理学，直到1970年代末，人们才充分认识到它与动态规划之间的联系。

</aside>

### 时序差分学习

突破性的学习方法之一被称为**时序差分（TD）**，它利用一种**自举（bootstrapping）**机制，在历史估计的基础上更新批评家函数。

- **Witten (1977)** 介绍了第一个 TD 学习算法，称为 **TD(0)**
- 在研究人员意识到时序差分和动态规划之间的内在联系之后，出现了一大类基于TD的算法
- **TD(λ)** 和 **TD(n)**：由 Richard Sutton 和 Andrew Barto 在1980年代开发

### Q-learning 和 SARSA

<aside>
🌟

迄今为止，最著名的 TD 算法可能是 **Q-learning**，它由 Chris Watkins 在**1989年**提出。实质上，Q-learning 是一种**离策略（off-policy）**算法，其中动作价值函数通过来自任意策略的样本进行优化。

</aside>

另一个重要的 TD 算法是 **SARSA**，它是一种**在策略（on-policy）**算法，其动作价值函数通过来自同一策略的样本来估计。

MC和TD方法都侧重于解决有限元马尔可夫任务，并用表格函数对可数空间进行建模。表格表示法阻碍了这些方法解决大规模和复杂的任务，例如自动驾驶和玩电脑游戏。

### 策略梯度方法

在**1990年代**，策略梯度方法开始引起强化学习社区的特别兴趣。待学习的策略由一个参数化函数来近似，该函数的参数沿着长期回报的梯度上升方向进行搜索。

其基本思想是将强化学习视为一个**随机优化问题**，因此其策略梯度与最优性条件无关。有三种流行的策略梯度：

1. **似然比梯度**（Likelihood Ratio Gradient）
2. **自然策略梯度**（Natural Policy Gradient）
3. **确定性策略梯度**（Deterministic Policy Gradient）

策略梯度算法形成了一个新的强化学习家族，称为**直接方法**。这些方法通常处理具有连续状态和动作空间的高维任务。

<aside>
⚠️

**挑战**：朴素版的策略梯度并不是解决实际强化学习问题的救星。一个主要挑战是基于样本的梯度估计存在**较大方差**。

</aside>

为了减少这种方差，通常会同时估计一个参数化的价值函数，这在收敛性和准确性方面带来了额外的好处。具有价值函数估计的方法可以进一步演变为著名的**行动者-批评家（Actor-Critic, AC）**架构。

### 行动者-批评家架构

<aside>
🎭

**AC 架构**学习近似策略和价值函数；这些函数分别被称为**"行动者"（actor）**和**"批评家"（critic）**。行动者和批评家以一种交织的方式相互影响：

- 行动者采取的动作由批评家评估
- 行动者根据批评家的评估改进策略
</aside>

**Barto 等人 (1983)** 可能介绍了第一个行动者-批评家算法，该算法采用线性函数作为批评家和行动者的近似。

**1992年**，IBM 展示了一个程序，该程序使用行动者-批评家算法来玩西洋双陆棋。该算法的技艺足以与最优秀的人类玩家相媲美。

### 深度强化学习

<aside>
🚀

**深度神经网络**的引入是强化学习历史上的另一个里程碑，它使强化学习能够解决许多以前难以解决的问题，例如直接从图像像素玩视频游戏。

</aside>

如今，深度学习和强化学习的结合正在彻底改变人工智能的方向，这被普遍认为是**迈向通用人工智能的关键一步**。

深度强化学习的重要成就：

- **Atari 视频游戏应用** (2015)：达到了超人水平的表现
- **AlphaGo** (2015)：第一个在19x19的棋盘上击败了人类职业棋手的围棋程序
- **AlphaGo Zero**：以89:11的比例战胜了其先前版本

---

## 1.2 强化学习应用示例

人们可以发现，强化学习和最优控制共享着非常相似的问题表述。

<aside>
📊

**总的来说**：

- **强化学习**更适合解决**大规模、复杂和随机**的问题，例如电脑游戏、围棋和自动驾驶
- **最优控制**通常面临**简单和确定性**的任务，这需要实时计算最优动作
</aside>

如今，由于从滚动时域控制的角度对它们之间联系的深刻理解，这两种技术之间的界限正在逐渐消失。

---

### 1.2.1 井字游戏

**井字游戏（Tic-tac-toe）**是一种供两名玩家玩的纸笔游戏。成功将自己的三个标记放置在水平、垂直或对角线行中的玩家赢得游戏。

### MENACE：第一个AI玩家

这款游戏的第一个AI玩家 **MENACE** 是使用304个火柴盒和一些9种不同颜色的珠子设计的。

- 每个火柴盒对应玩家可能面临的一种独特的棋盘布局
- 火柴盒中包含一些彩色珠子，对应于下一步可以采取的每一种可能的走法
- 当 MENACE 走棋时，操作员只需选择与当前棋盘布局相对应的火柴盒，然后随机抽取一颗珠子来走棋

**学习机制**：

- ✅ 每当 MENACE 赢得一场游戏时，就在每个下过的火柴盒中添加一颗额外的珠子
- ❌ 每当 MENACE 输掉一场游戏时，就移除一颗对应于失败走法的珠子

### 现代强化学习方法

现代强化学习方法已被应用于井字游戏。在这些方法中：

- **游戏状态**：指的是在 3x3 网格上 "o" 和 "x" 的放置情况
- **状态空间**：井字游戏总共有 **19683** 个状态
- **优化**：许多状态在旋转或反射上是相同的，有些状态甚至无法达到，它们被从状态空间中移除以降低计算复杂性

**状态值估计**：

每个状态被分配一个值，以表示从该状态开始的获胜概率：

- 所有 "o" 连成三行的状态：获胜概率为 **100%**
- 所有 "x" 连成三行的状态：获胜概率为 **0%**
- 在任何一种情况下，回合都将终止，并宣布获胜者

状态值（即每个状态的获胜可能性）的估计是通过**试错过程**学习的。代理创建一个状态树，表示从当前状态开始所有可能的后续状态，然后逐一尝试所有可能的走法，以观察收到的奖励：

- 赢了：**+1**
- 输了：**-1**
- 平局：**0**

---

### 1.2.2 围棋

**围棋（Chinese Go）**是一种棋盘游戏，两名玩家在 19×19 的网格棋盘上竞争控制更多领土。

- 网格棋盘最多包含 **361** 个点
- 玩家试图用自己的棋子包围棋盘上的空白交叉点
- 根据包围的空间数量获得分数
- 分数较高的玩家赢得游戏

<aside>
🌌

**复杂度**：与国际象棋程序不同，由于围棋巨大的状态空间，不可能用搜索技术来解决它。围棋中可能的动作序列数量约为 $10^{170}$，这比宇宙中所有原子的数量还要多！

</aside>

这个游戏是如此复杂，以至于即使是最有成就的围棋玩家也可能难以判断某些走法是好是坏。这就是为什么程序员很难编写出一个好的围棋策略。

### AlphaGo 的突破

最近，具有神经网络近似的强化学习在下围棋方面取得了突破性的表现。

**2016年**，Silver 和他的同事开发了一种用于下围棋的深度Q网络方法，称为 **AlphaGo**。

**技术架构**：

在这种方法中，棋盘布局（一个 19×19 的图像）被发送到两个卷积神经网络：

1. **价值网络**：评估特定棋盘布局的好坏
2. **策略网络**：用于采样动作

**训练过程**：

- 策略网络首先用**人类专家的走法**进行初始化
- 然后通过强化学习器沿着**最陡峭的上升方向**进行更新
- 同时，价值网络被训练来预测游戏输赢的可能性
- 这两个网络通过**蒙特卡洛树搜索**在逻辑上结合起来，以实现更有效的探索和策略部署

<aside>
🏆

**历史性时刻**：当使用上述方法的 AlphaGo 以**四比一**的战绩击败了世界著名的职业围棋棋手**李世石**时，全世界都为之震惊。

</aside>

**AlphaGo Zero**：后来，一个新版本，称为 AlphaGo Zero，取得了更令人惊讶的表现，它以**89:11**的比例战胜了其先前版本。

---

### 1.2.3 自动驾驶汽车

**自动驾驶**是人工智能领域最突出的应用之一。随着深度学习算法和高性能处理器的最新进展，无人驾驶汽车已经从仅在科幻材料中出现，发展到现实世界中的原型。

### 为什么使用强化学习？

自动驾驶系统可以用监督学习方法进行训练，但这需要大量的专家驾驶数据。

<aside>
💡

**优势**：通过使用强化学习来训练自动驾驶策略，可以在**没有昂贵的专家数据**的情况下探索未知的交通场景。此外，在线强化学习训练是开发类似于人类驾驶员的**自我进化能力**的一种有前途的方式。

</aside>

### 实际应用案例

在过去十年中，已经通过虚拟模拟器开发了一些自学自动驾驶能力的例子。

**Peng 等人 (2021)** 在 TORCS 环境中开发了一种端到端的自动驾驶算法：

- 使汽车能够在**没有外部教师**的情况下学会如何保持在车道内
- 自动驾驶策略包括一个具有**三个卷积层**和**三个全连接层**的深度神经网络

**工作原理**：

- **输入**：由单个前置摄像头捕获的图像
- **奖励机制**：
    - 当汽车保持在车道内时：获得**正奖励**
    - 当汽车驶出车道时：收到**高额的负奖励**

**训练结果**可视化为显著图，显示自学策略更关注**车道线**和**前方道路的视野**。

<aside>
🧠

有趣的是，这种自学行为在某种程度上与**人类驾驶员的驾驶知识**相匹配。这个例子表明，强化学习代理具有一种**自我进化能力**，可以自动发展出高级驾驶智能。

</aside>

---

## 1.3 当今强化学习的主要挑战

狭义上，**强化学习是机器学习的一个子领域**。它采取适当的行动以在特定情况下最大化奖励。

### 机器学习的三大类别

| **类别** | **特点** |
| --- | --- |
| **监督学习** | 从由知识渊博的外部教师标记的一组训练数据中学习 |
| **无监督学习** | 主要涉及在未标记数据集合中寻找隐藏模式 |
| **强化学习** | 通过与环境进行试错交互来学习 |

<aside>
🔗

**广义上**，强化学习与**最优控制**有很强的联系。两种方法都在受到某种环境动力学约束的同时，寻求优化（最大化或最小化）某个性能指标。

</aside>

**区别**：

- **最优控制**：通常需要一个精确的模型，并对形式主义和确定性做出一些假设
- **强化学习**：通常是无模型的，需要从未知环境中收集经验

---

### 六大核心挑战

尽管取得了一些成功，但在实际应用中部署强化学习时仍面临各种挑战：

---

### 1.3.1 探索-利用困境

<aside>
⚖️

几乎所有的RL方法都面临一个根本性挑战，即如何在**探索**和**利用**之间进行权衡。

</aside>

**困境描述**：

- 代理尝试各种动作与环境互动，并且必须利用先前的经验尽快确定一个策略
- 同时，代理还需要探索其他动作以寻求更好的决策

**问题**：

- ❌ 过分依赖先前的经验可能会牺牲探索新动作的能力
- ❌ 过多的新探索将给策略搜索带来更多随机性

策略随机性可以增加探索能力，但会在某种程度上对策略的最优性产生负面影响。

**极端情况：离线强化学习**

在某些极端情况下，甚至可能禁止环境探索，只能访问预先收集的历史数据集。在这些条件下的强化学习算法被称为**离线强化学习（offline RL）**，其中仅有先前经验可用于策略学习。

---

### 1.3.2 不确定性与部分可观测性

<aside>
👁️

经典的RL要求**完全可观测性**，并且不能轻易扩展到部分可观测的任务中。在实践中，代理可能无法观测到所有状态。

</aside>

**实际例子**：

由于车辆间的遮挡或传感器范围限制，自动驾驶汽车无法检测到交通环境中的所有周围车辆。

**挑战**：

- **部分可观测马尔可夫决策过程**在计算上更加棘手
- 必须估计隐藏状态作为决策的输入信息
- 对环境动力学的任何描述都不可避免地具有一定的**模型失配**
- 大的模型误差给部分可观测问题带来了额外的挑战，这需要一个**鲁棒的最优策略**

---

### 1.3.3 时间延迟奖励

<aside>
⏰

在许多回合制任务中，**稀疏的奖励信号**（例如，游戏中是赢还是输）会很晚才传递，甚至在游戏结束时才传递。

</aside>

**问题**：

- 时间延迟的奖励随着时间的推移被日益稀释
- 它从遥远的先前状态带来的信息非常少
- 因此，在这种回合制任务中，经常会发生**收敛速度非常慢**和**策略准确性差**的情况

**后果**：

必须执行大量昂贵的环境交互，以从先前状态传播延迟的强化信号。

**解决方案**：

一种解决方案是添加一些中间信号以减少奖励的稀疏性，但这需要适当的**奖励重塑技术**。

---

### 1.3.4 安全约束导致的不可行性

<aside>
🚨

满足状态约束的挑战来自其导致的**不可行性问题**。受约束强化学习中的不可行性行为非常复杂，因为它涉及两个相互关联的任务：

1. 找到一个最优策略
2. 确定其可行区域
</aside>

**复杂性**：

- 如果一个策略没有在可行区域中定义，那么它就是无意义的
- 一个区域的可行性也取决于如何选择其策略
- 这两个任务相互之间紧密耦合，这就是为什么受约束强化学习难以处理的原因

**安全保证的挑战**：

如果要求安全性，这种不可行性问题会变得更加严重。安全保证意味着学习到的策略应**严格满足硬状态约束**。

为了保证绝对安全：

- 必须探索可行区域内部和外部的环境信息
- 在代理与环境的交互过程中不得引入任何危险的约束违反

---

### 1.3.5 非平稳环境

<aside>
🌊

几乎所有的学习者都使用**平稳数据**，在**独立同分布（iid）数据**的假设下训练模型。同样，强化学习也需要一个平稳的环境来学习策略。

</aside>

**挑战**：

- 某些任务可能涉及随时间变化的结构或参数
- 如果环境变化太快，强化学习很可能由于缺乏 iid 数据而失败
- 在真实环境中，这一挑战无法轻易缓解

**内在不稳定性**：

更糟糕的是，如果某些内在不稳定性的原因没有被正确消除，强化学习算法可能在平稳环境中也会失败。

**"死亡三元组"问题**：

一个典型的不稳定例子来自**"死亡三元组"（deadly triad）**问题，即以下三者的同时存在：

1. **自举**（bootstrapping）
2. **离策略**（off-policy）
3. **函数逼近**（function approximation）

---

### 1.3.6 缺乏泛化性

<aside>
🔄

**适应未见过的场景**是人类智能的一个特征。然而，现有的强化学习方法通常**缺乏泛化性**。

</aside>

**问题**：

- 尽管当今的强化学习方法可以学会解决复杂的任务，但它们倾向于**专门化于其训练领域**
- 在部署到不同场景时经常会崩溃
- 这给将强化学习应用于现实世界应用带来了巨大挑战

**"模拟到现实"（Sim-to-Real）问题**：

在许多任务中，强化学习策略首先在**模拟环境**中训练，然后部署到**现实世界应用**中使用。

当模拟器的保真度相对较低时，就会出现严重的"模拟到现实"的泛化问题。需要**域适应（domain adaptation）**等技术来计算可迁移的策略以获得更好的泛化性。

---